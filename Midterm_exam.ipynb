{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\core\\indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import log_loss, roc_auc_score, auc, roc_curve\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from IPython.display import display\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_ROOT = \"data/\"\n",
    "#讀取數據\n",
    "dfoff = pd.read_csv(os.path.join(DATA_ROOT,'train_offline.csv'))\n",
    "dftest = pd.read_csv(os.path.join(DATA_ROOT,'test_offline.csv'))\n",
    "dftest = dftest[~dftest.Coupon_id.isna()]\n",
    "dftest.reset_index(drop=True, inplace=True)\n",
    "# 檢查 DataFrame 空缺值的狀態\n",
    "def na_check(df_data):\n",
    "    data_na = (df_data.isnull().sum() / len(df_data)) * 100\n",
    "    data_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)\n",
    "    missing_data = pd.DataFrame({'Missing Ratio' :data_na})\n",
    "    display(missing_data.head(10))\n",
    "# Creat target label \n",
    "\"\"\"\n",
    "According to the definition, \n",
    "1) buy with coupon within (include) 15 days ==> 1\n",
    "2) buy with coupon but out of 15 days ==> 0\n",
    "3) buy without coupon ==> -1 (we don't care)\n",
    "\"\"\"\n",
    "def label(row):\n",
    "    if np.isnan(row['Coupon_id']):\n",
    "        return -1\n",
    "    if not np.isnan(row['Date']):\n",
    "        td = pd.to_datetime(row['Date'], format='%Y%m%d') -  pd.to_datetime(row['Date_received'], format='%Y%m%d')\n",
    "        if td <= pd.Timedelta(15, 'D'):\n",
    "            return 1\n",
    "    return 0\n",
    "#從字串轉成浮點數\n",
    "def convertRate(row):\n",
    "    \"\"\"Convert discount to rate\"\"\"\n",
    "    if row == 'null':\n",
    "        return 1.0\n",
    "    elif ':' in row:\n",
    "        rows = row.split(':')\n",
    "        return 1.0 - float(rows[1])/float(rows[0])\n",
    "    else:\n",
    "        return float(row)\n",
    "#轉成星期\n",
    "def getWeekday(row):\n",
    "    if (np.isnan(row)) or (row==-1):\n",
    "        return row\n",
    "    else:\n",
    "        return pd.to_datetime(row, format = \"%Y%m%d\").dayofweek+1 # add one to make it from 0~6-> 1~7\n",
    "#增加特徵期望值\n",
    "#1.距離越遠 期望值越低 \n",
    "#2.折扣越高 期望值越低\n",
    "def getExpecteValue(row):\n",
    "    result = 0.0\n",
    "    temp = 0.0\n",
    "    if row['Distance'] == 0:\n",
    "       temp = 0.1\n",
    "    else:\n",
    "       temp = row['Distance']\n",
    "    result = (1/row['Discount_rate'])*(1/temp)\n",
    "    return result\n",
    "#產生lable\n",
    "dfoff[\"label\"] = dfoff.apply(label, axis=1)\n",
    "#--------------------------------------------------\n",
    "#label = -1 刪掉數據\n",
    "train_x = dfoff[dfoff[\"label\"] != -1].drop([\"Date\",\"label\"], axis = 1).copy()\n",
    "train_y = dfoff[\"label\"][dfoff[\"label\"] != -1].copy()\n",
    "#---------------------------\n",
    "#合併訓練集及測試集\n",
    "df = pd.concat([train_x, dftest],sort=True)\n",
    "#----------------------------------------\n",
    "#資料預處理 補缺失值及特徵轉換\n",
    "df[\"Discount_rate\"] = df['Discount_rate'].astype('str').apply(convertRate)\n",
    "df[\"Distance\"][df[\"Distance\"].isnull()] = df[\"Distance\"][~df[\"Distance\"].isnull()].median()\n",
    "df[\"Discount_rate\"][df[\"Discount_rate\"]< 0.7] = df[\"Discount_rate\"][~df[\"Discount_rate\"].isnull()].median()\n",
    "df[\"Discount_rate_square\"] = df[\"Discount_rate\"].apply(lambda x:x*x)\n",
    "df[\"Discount_rate_third\"] = df[\"Discount_rate_square\"].apply(lambda x:x*x)\n",
    "df[\"Distance_square\"] = df[\"Distance\"].apply(lambda x:x*x)\n",
    "df[\"Discount_third\"] = df[\"Distance_square\"].apply(lambda x:x*x)\n",
    "df['weekday'] = df['Date_received'].apply(getWeekday)\n",
    "df['Expect_Value'] = df.apply(getExpecteValue,axis = 1)\n",
    "#-------------------------------------------------------\n",
    "#以下為離散化 及 ONE_HOT_ENCODING \n",
    "df[\"C_W_D\"] = pd.cut(df[\"Distance\"], [0,1,2,4,9,10])\n",
    "df = pd.get_dummies(df, columns = [\"C_W_D\"], prefix=\"C_W_D\")\n",
    "df[\"C_W_DR\"] = pd.cut(df[\"Discount_rate\"], [0.3,0.72,0.82,0.85,0.87,0.9,1.0])\n",
    "df = pd.get_dummies(df, columns = [\"C_W_DR\"], prefix=\"C_W_DR\")\n",
    "df['received_month'] = df['Date_received'].apply(lambda x: pd.to_datetime(x, format = \"%Y%m%d\").month)\n",
    "df = pd.get_dummies(df, columns = [\"received_month\"], prefix=\"received_month\")\n",
    "df['received_day'] = df['Date_received'].apply(lambda x: pd.to_datetime(x, format = \"%Y%m%d\").day)\n",
    "df[\"C_R_D\"] = pd.cut(df[\"received_day\"], [0,5,10,15,20,25,30])\n",
    "df = pd.get_dummies(df, columns = [\"C_R_D\"], prefix=\"C_R_D\")\n",
    "#---------------------------\n",
    "#特徵交叉\n",
    "Expect_Value_mean = df.groupby(['User_id'])['Expect_Value'].mean().reset_index()\n",
    "Expect_Value_mean.columns = ['User_id', 'Expect_Value_mean']\n",
    "Discount_rate_Usermean = df.groupby(['User_id'])['Discount_rate'].mean().reset_index()\n",
    "Discount_rate_Usermean.columns = ['User_id','Discount_rate_Usermean']\n",
    "Discount_rate_Squaremean = df.groupby(['User_id'])['Discount_rate_square'].mean().reset_index()\n",
    "Discount_rate_Squaremean.columns = ['User_id','Discount_rate_Squaremean']\n",
    "Discount_Squaremean = df.groupby(['User_id'])['Distance_square'].mean().reset_index()\n",
    "Discount_Squaremean.columns = ['User_id','Discount_Squaremean']\n",
    "Discount_rate_thirdmean = df.groupby(['User_id'])['Discount_rate_third'].mean().reset_index()\n",
    "Discount_rate_thirdmean.columns = ['User_id','Discount_rate_thirdmean']\n",
    "Discount_thirdmean = df.groupby(['User_id'])['Discount_third'].mean().reset_index()\n",
    "Discount_thirdmean.columns = ['User_id','Discount_thirdmean']\n",
    "temp = pd.merge(Discount_rate_Usermean, Expect_Value_mean, how='left', on=['User_id'])\n",
    "temp = pd.merge(temp, Discount_rate_Squaremean, how='left', on=['User_id'])\n",
    "temp = pd.merge(temp, Discount_Squaremean, how='left', on=['User_id'])\n",
    "temp = pd.merge(temp, Discount_rate_thirdmean, how='left', on=['User_id'])\n",
    "temp = pd.merge(temp, Discount_thirdmean, how='left', on=['User_id'])\n",
    "df = pd.merge(df,temp, how='left', on=['User_id'])\n",
    "Expect_Value_sum = df.groupby(['Date_received','Coupon_id'])['Expect_Value'].sum().reset_index()\n",
    "Expect_Value_sum.columns = ['Date_received','Coupon_id','Expect_Value_sum']\n",
    "Expect_Value_mean = df.groupby(['Date_received','Coupon_id'])['Expect_Value'].mean().reset_index()\n",
    "Expect_Value_mean.columns = ['Date_received','Coupon_id','Expect_Value_sum_user']\n",
    "Expect_Value_mean_user = df.groupby(['User_id','Coupon_id'])['Expect_Value'].mean().reset_index()\n",
    "Expect_Value_mean_user.columns = ['User_id','Coupon_id','Expect_Value_mean_user']\n",
    "Expect_Value_sum_user = df.groupby(['User_id','Coupon_id'])['Expect_Value'].sum().reset_index()\n",
    "Expect_Value_sum_user.columns = ['User_id','Coupon_id','Expect_Value_sum']\n",
    "temp1 = pd.merge(Expect_Value_mean,Expect_Value_sum, how='left', on=['Date_received','Coupon_id'])\n",
    "temp2 = pd.merge(Expect_Value_mean_user,Expect_Value_sum_user, how='left', on=['User_id','Coupon_id'])\n",
    "df = pd.merge(df,temp1, how='left', on=['Date_received','Coupon_id'])\n",
    "df = pd.merge(df,temp2, how='left', on=['User_id','Coupon_id'])\n",
    "Discount_rate_MerchantSum = df.groupby(['Merchant_id'])['Discount_rate'].sum().reset_index()\n",
    "Discount_rate_MerchantSum.columns = ['Merchant_id', 'Discount_rate_MerchantSum']\n",
    "df =pd.merge(df, Discount_rate_MerchantSum, how='left', on=['Merchant_id'])\n",
    "Discount_rate_MerchantMean = df.groupby(['Merchant_id'])['Discount_rate'].mean().reset_index()\n",
    "Discount_rate_MerchantMean.columns = ['Merchant_id', 'Discount_rate_MerchantMean']\n",
    "df =pd.merge(df, Discount_rate_MerchantMean, how='left', on=['Merchant_id'])\n",
    "#-----------------------------------------------------------------------------\n",
    "# df['Merchant_id']= LabelEncoder().fit_transform(df['Merchant_id'])\n",
    "# df['User_id']= LabelEncoder().fit_transform(df['User_id'])\n",
    "# df['Coupon_id']= LabelEncoder().fit_transform(df['Coupon_id'])\n",
    "#-----------------------------------------\n",
    "# print(df.shape)\n",
    "# 將資料最大最小化\n",
    "df = MinMaxScaler().fit_transform(df)\n",
    "train_num = train_y.shape[0]\n",
    "real_train_x = df[:train_num]\n",
    "real_test_x = df[train_num:]\n",
    "#以下為用隨機森林做特徵選取\n",
    "# 建立模型 (使用 20 顆樹，每棵樹的最大深度為 4)\n",
    "# clf = RandomForestClassifier(n_estimators=20,max_depth = 4)\n",
    "\n",
    "# 訓練模型\n",
    "# clf.fit(real_train_x, train_y)\n",
    "# threshold = 0.1\n",
    "# importances = clf.feature_importances_\n",
    "# print(importances)\n",
    "# df_select = df[:, importances > threshold]\n",
    "# print(df_select.shape[1])\n",
    "# real_train_x= df_select[:train_num]\n",
    "# real_test_x = df_select[train_num:]\n",
    "\n",
    "#以下為模型擬合資料+輸出預測結果\n",
    "lr = LogisticRegression(tol=0.001, penalty='l2', fit_intercept=True, C=1.0)\n",
    "rf = RandomForestClassifier(n_estimators=100, min_samples_split=2, min_samples_leaf=1, \n",
    "                            max_features='sqrt', max_depth=6, bootstrap=True)\n",
    "xgb = XGBClassifier(n_estimators = 100,min_child_weight = 5, max_depth = 5,subsample = 0.7,learning_rate = 0.2)\n",
    "meta_estimator = XGBClassifier(n_estimators = 50,min_child_weight = 3, max_depth = 3,subsample = 0.8,learning_rate = 0.3)\n",
    "sclf = StackingClassifier(classifiers=[lr, xgb, rf],\n",
    "                          use_probas=True,\n",
    "                          average_probas=False,\n",
    "                          meta_classifier=meta_estimator)\n",
    "sclf.fit(real_train_x, train_y)\n",
    "y_test_pred = sclf.predict_proba(real_test_x)\n",
    "# modelfit = xgb.fit(real_train_x,train_y)\n",
    "# accuracy = np.mean(cross_val_score(modelfit, real_train_x, train_y, cv=3, scoring='accuracy'))\n",
    "# print(\"XGBClassifier accuracy: \", accuracy)\n",
    "# y_test_pred = modelfit.predict_proba(real_test_x)\n",
    "test1 = dftest.loc[:,['Discount_rate','Expect_Value']].copy()\n",
    "test1['pred_prob'] = y_test_pred[:, 1]\n",
    "output = pd.concat((dftest[[\"User_id\", \"Coupon_id\", \"Date_received\"]], test1[\"pred_prob\"]), axis=1)\n",
    "output.loc[:, \"User_id\"] = output[\"User_id\"].apply(lambda x:str(int(x)))\n",
    "output.loc[:, \"Coupon_id\"] = output[\"Coupon_id\"].apply(lambda x:str(int(x)))\n",
    "output.loc[:, \"Date_received\"] = output[\"Date_received\"].apply(lambda x:str(int(x)))\n",
    "output[\"uid\"] = output[[\"User_id\", \"Coupon_id\", \"Date_received\"]].apply(lambda x: '_'.join(x.values), axis=1)\n",
    "output.reset_index(drop=True, inplace=True)\n",
    "out = output.groupby(\"uid\", as_index=False).mean()\n",
    "out = out[[\"uid\", \"pred_prob\"]]\n",
    "out.columns = [\"uid\", \"label\"]\n",
    "out.to_csv(\"StackingClassifier_Modify_2.csv\", header=[\"uid\", \"label\"], index=False) # submission format\n",
    "#-----------------------------------------------------------------------------------\n",
    "#以下為資料視覺化+數據清洗\n",
    "# df = pd.merge(df, Expect_Value_mean, how='left', on=['User_id'])\n",
    "# df.head(10)\n",
    "# print(df['User_id'].nunique())\n",
    "# print(df['Merchant_id'].nunique())\n",
    "# print(df['Coupon_id'].nunique())\n",
    "# df['Expect_Value_log'] = np.log1p(df[\"Expect_Value\"])\n",
    "# df['Distance_log'] = np.log1p(df['Distance'])\n",
    "# df['received_month'].hist()\n",
    "# plt.show()\n",
    "# df['received_day'].hist()\n",
    "# plt.show()\n",
    "# plt.boxplot(df['Expect_Value_log'],\n",
    "#             notch=True,  # notch shape\n",
    "#             sym='bs',     # blue squares for outliers\n",
    "#             vert=True,   # vertical box aligmnent\n",
    "#             patch_artist=True)   # fill with color \n",
    "# plt.title('Expect_Value_log') \n",
    "# plt.show()\n",
    "\n",
    "\n",
    "#遇到均值編碼時 連id也一起算平均 所以merge不起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import log_loss, roc_auc_score, auc, roc_curve\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from IPython.display import display\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_ROOT = \"data/\"\n",
    "#讀取數據\n",
    "dfoff = pd.read_csv(os.path.join(DATA_ROOT,'train_offline.csv'))\n",
    "dftest = pd.read_csv(os.path.join(DATA_ROOT,'test_offline.csv'))\n",
    "dftest = dftest[~dftest.Coupon_id.isna()]\n",
    "dftest.reset_index(drop=True, inplace=True)\n",
    "# 檢查 DataFrame 空缺值的狀態\n",
    "def na_check(df_data):\n",
    "    data_na = (df_data.isnull().sum() / len(df_data)) * 100\n",
    "    data_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)\n",
    "    missing_data = pd.DataFrame({'Missing Ratio' :data_na})\n",
    "    display(missing_data.head(10))\n",
    "# Creat target label \n",
    "\"\"\"\n",
    "According to the definition, \n",
    "1) buy with coupon within (include) 15 days ==> 1\n",
    "2) buy with coupon but out of 15 days ==> 0\n",
    "3) buy without coupon ==> -1 (we don't care)\n",
    "\"\"\"\n",
    "#產生label\n",
    "def label(row):\n",
    "    if np.isnan(row['Coupon_id']):\n",
    "        return -1\n",
    "    if not np.isnan(row['Date']):\n",
    "        td = pd.to_datetime(row['Date'], format='%Y%m%d') -  pd.to_datetime(row['Date_received'], format='%Y%m%d')\n",
    "        if td <= pd.Timedelta(15, 'D'):\n",
    "            return 1\n",
    "    return 0\n",
    "#字串轉成比率\n",
    "def convertRate(row):\n",
    "    \"\"\"Convert discount to rate\"\"\"\n",
    "    if row == 'null':\n",
    "        return 1.0\n",
    "    elif ':' in row:\n",
    "        rows = row.split(':')\n",
    "        return 1.0 - float(rows[1])/float(rows[0])\n",
    "    else:\n",
    "        return float(row)\n",
    "# Generate features - weekday acquired coupon\n",
    "def getWeekday(row):\n",
    "    if (np.isnan(row)) or (row==-1):\n",
    "        return row\n",
    "    else:\n",
    "        return pd.to_datetime(row, format = \"%Y%m%d\").dayofweek+1 # add one to make it from 0~6-> 1~7\n",
    "#增加特徵期望值   \n",
    "#1.距離越遠 期望值越低 \n",
    "#2.折扣越高 期望值越低\n",
    "def getExpecteValue(row):\n",
    "    result = 0.0\n",
    "    temp = 0.0\n",
    "    if row['Distance'] == 0:\n",
    "       temp = 0.1\n",
    "    else:\n",
    "       temp = row['Distance']\n",
    "    result = (1/row['Discount_rate'])*(1/temp)\n",
    "    return result  \n",
    "#產生label\n",
    "dfoff[\"label\"] = dfoff.apply(label, axis=1)\n",
    "#label = -1 刪掉數據\n",
    "train_x = dfoff[dfoff[\"label\"] != -1].drop([\"Date\",\"label\"], axis = 1).copy()\n",
    "train_y = dfoff[\"label\"][dfoff[\"label\"] != -1].copy()\n",
    "#------------------------------------------\n",
    "#資料預處理 補缺失值及特徵轉換\n",
    "df = pd.concat([train_x, dftest],sort=True)\n",
    "df[\"Discount_rate\"] = df['Discount_rate'].astype('str').apply(convertRate)\n",
    "df[\"Distance\"][df[\"Distance\"].isnull()] = df[\"Distance\"][~df[\"Distance\"].isnull()].median()\n",
    "df[\"Discount_rate\"][df[\"Discount_rate\"]< 0.7] = df[\"Discount_rate\"][~df[\"Discount_rate\"].isnull()].median()\n",
    "df['weekday'] = df['Date_received'].apply(getWeekday)\n",
    "df['Expect_Value'] = df.apply(getExpecteValue,axis = 1)\n",
    "#-----------------------------------------------------------\n",
    "#以下為離散化 AND ONE_HOT_ENCODING \n",
    "df[\"C_W_D\"] = pd.cut(df[\"Distance\"], [0,1,2,4,9,10])\n",
    "df = pd.get_dummies(df, columns = [\"C_W_D\"], prefix=\"C_W_D\")\n",
    "df[\"C_W_DR\"] = pd.cut(df[\"Discount_rate\"], [0.3,0.72,0.82,0.85,0.87,0.9,1.0])\n",
    "df = pd.get_dummies(df, columns = [\"C_W_DR\"], prefix=\"C_W_DR\")\n",
    "df['received_month'] = df['Date_received'].apply(lambda x: pd.to_datetime(x, format = \"%Y%m%d\").month)\n",
    "df = pd.get_dummies(df, columns = [\"received_month\"], prefix=\"received_month\")\n",
    "df['received_day'] = df['Date_received'].apply(lambda x: pd.to_datetime(x, format = \"%Y%m%d\").day)\n",
    "df[\"C_R_D\"] = pd.cut(df[\"received_day\"], [0,5,10,15,20,25,30])\n",
    "df = pd.get_dummies(df, columns = [\"C_R_D\"], prefix=\"C_R_D\")\n",
    "#---------------------------\n",
    "#特徵交叉\n",
    "Expect_Value_mean = df.groupby(['User_id'])['Expect_Value'].mean().reset_index()\n",
    "Expect_Value_mean.columns = ['User_id', 'Expect_Value_mean']\n",
    "Discount_rate_Usermean = df.groupby(['User_id'])['Discount_rate'].mean().reset_index()\n",
    "Discount_rate_Usermean.columns = ['User_id','Discount_rate_Usermean']\n",
    "temp = pd.merge(Discount_rate_Usermean, Expect_Value_mean, how='left', on=['User_id'])\n",
    "df = pd.merge(df,temp, how='left', on=['User_id'])\n",
    "Expect_Value_sum = df.groupby(['Date_received','Coupon_id'])['Expect_Value'].sum().reset_index()\n",
    "Expect_Value_sum.columns = ['Date_received','Coupon_id','Expect_Value_sum']\n",
    "Expect_Value_mean = df.groupby(['Date_received','Coupon_id'])['Expect_Value'].mean().reset_index()\n",
    "Expect_Value_mean.columns = ['Date_received','Coupon_id','Expect_Value_sum_user']\n",
    "Expect_Value_mean_user = df.groupby(['User_id','Coupon_id'])['Expect_Value'].mean().reset_index()\n",
    "Expect_Value_mean_user.columns = ['User_id','Coupon_id','Expect_Value_mean_user']\n",
    "Expect_Value_sum_user = df.groupby(['User_id','Coupon_id'])['Expect_Value'].sum().reset_index()\n",
    "Expect_Value_sum_user.columns = ['User_id','Coupon_id','Expect_Value_sum']\n",
    "temp1 = pd.merge(Expect_Value_mean,Expect_Value_sum, how='left', on=['Date_received','Coupon_id'])\n",
    "temp2 = pd.merge(Expect_Value_mean_user,Expect_Value_sum_user, how='left', on=['User_id','Coupon_id'])\n",
    "df = pd.merge(df,temp1, how='left', on=['Date_received','Coupon_id'])\n",
    "df = pd.merge(df,temp2, how='left', on=['User_id','Coupon_id'])\n",
    "Discount_rate_MerchantSum = df.groupby(['Merchant_id'])['Discount_rate'].sum().reset_index()\n",
    "Discount_rate_MerchantSum.columns = ['Merchant_id', 'Discount_rate_MerchantSum']\n",
    "df =pd.merge(df, Discount_rate_MerchantSum, how='left', on=['Merchant_id'])\n",
    "Discount_rate_MerchantMean = df.groupby(['Merchant_id'])['Discount_rate'].mean().reset_index()\n",
    "Discount_rate_MerchantMean.columns = ['Merchant_id', 'Discount_rate_MerchantMean']\n",
    "df =pd.merge(df, Discount_rate_MerchantMean, how='left', on=['Merchant_id'])\n",
    "df['Merchant_id']= LabelEncoder().fit_transform(df['Merchant_id'])\n",
    "df['User_id']= LabelEncoder().fit_transform(df['User_id'])\n",
    "df['Coupon_id']= LabelEncoder().fit_transform(df['Coupon_id'])\n",
    "#-----------------------------------------\n",
    "# print(df.shape)\n",
    "# 將資料最大最小化\n",
    "df = MinMaxScaler().fit_transform(df)\n",
    "train_num = train_y.shape[0]\n",
    "real_train_x = df[:train_num]\n",
    "real_test_x = df[train_num:]\n",
    "#以下為用隨機森林做特徵選取\n",
    "# 建立模型 (使用 20 顆樹，每棵樹的最大深度為 4)\n",
    "# clf = RandomForestClassifier(n_estimators=20,max_depth = 4)\n",
    "\n",
    "# 訓練模型\n",
    "# clf.fit(real_train_x, train_y)\n",
    "# threshold = 0.1\n",
    "# importances = clf.feature_importances_\n",
    "# print(importances)\n",
    "# df_select = df[:, importances > threshold]\n",
    "# print(df_select.shape[1])\n",
    "# real_train_x= df_select[:train_num]\n",
    "# real_test_x = df_select[train_num:]\n",
    "\n",
    "# #以下為模型擬合資料+輸出預測結果\n",
    "lr = LogisticRegression(tol=0.001, penalty='l2', fit_intercept=True, C=1.0)\n",
    "rf = RandomForestClassifier(n_estimators=100, min_samples_split=2, min_samples_leaf=1, \n",
    "                            max_features='sqrt', max_depth=6, bootstrap=True)\n",
    "xgb = XGBClassifier(n_estimators = 100,min_child_weight = 5, max_depth = 5,subsample = 0.7,learning_rate = 0.2)\n",
    "meta_estimator = XGBClassifier(n_estimators = 50,min_child_weight = 3, max_depth = 3,subsample = 0.8,learning_rate = 0.3)\n",
    "sclf = StackingClassifier(classifiers=[lr, xgb, rf],\n",
    "                          use_probas=True,\n",
    "                          average_probas=False,\n",
    "                          meta_classifier=meta_estimator)\n",
    "sclf.fit(real_train_x, train_y)\n",
    "y_test_pred = sclf.predict_proba(real_test_x)\n",
    "# modelfit = xgb.fit(real_train_x,train_y)\n",
    "# accuracy = np.mean(cross_val_score(modelfit, real_train_x, train_y, cv=3, scoring='accuracy'))\n",
    "# print(\"XGBClassifier accuracy: \", accuracy)\n",
    "# y_test_pred = modelfit.predict_proba(real_test_x)\n",
    "test1 = dftest.loc[:,['Discount_rate','Expect_Value']].copy()\n",
    "test1['pred_prob'] = y_test_pred[:, 1]\n",
    "output = pd.concat((dftest[[\"User_id\", \"Coupon_id\", \"Date_received\"]], test1[\"pred_prob\"]), axis=1)\n",
    "output.loc[:, \"User_id\"] = output[\"User_id\"].apply(lambda x:str(int(x)))\n",
    "output.loc[:, \"Coupon_id\"] = output[\"Coupon_id\"].apply(lambda x:str(int(x)))\n",
    "output.loc[:, \"Date_received\"] = output[\"Date_received\"].apply(lambda x:str(int(x)))\n",
    "output[\"uid\"] = output[[\"User_id\", \"Coupon_id\", \"Date_received\"]].apply(lambda x: '_'.join(x.values), axis=1)\n",
    "output.reset_index(drop=True, inplace=True)\n",
    "out = output.groupby(\"uid\", as_index=False).mean()\n",
    "out = out[[\"uid\", \"pred_prob\"]]\n",
    "out.columns = [\"uid\", \"label\"]\n",
    "out.to_csv(\"StackingClassifier.csv\", header=[\"uid\", \"label\"], index=False) # submission format\n",
    "#-----------------------------------------------------------------------------------\n",
    "#以下為資料視覺化+數據清洗\n",
    "# df = pd.merge(df, Expect_Value_mean, how='left', on=['User_id'])\n",
    "# df.head(10)\n",
    "# print(df['User_id'].nunique())\n",
    "# print(df['Merchant_id'].nunique())\n",
    "# print(df['Coupon_id'].nunique())\n",
    "# df['Expect_Value_log'] = np.log1p(df[\"Expect_Value\"])\n",
    "# df['Distance_log'] = np.log1p(df['Distance'])\n",
    "# df['received_month'].hist()\n",
    "# plt.show()\n",
    "# df['received_day'].hist()\n",
    "# plt.show()\n",
    "# plt.boxplot(df['Expect_Value_log'],\n",
    "#             notch=True,  # notch shape\n",
    "#             sym='bs',     # blue squares for outliers\n",
    "#             vert=True,   # vertical box aligmnent\n",
    "#             patch_artist=True)   # fill with color \n",
    "# plt.title('Expect_Value_log') \n",
    "# plt.show()\n",
    "\n",
    "\n",
    "#遇到均值編碼時 連id也一起算平均 所以merge不起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\core\\indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import log_loss, roc_auc_score, auc, roc_curve\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from IPython.display import display\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_ROOT = \"data/\"\n",
    "#讀取數據\n",
    "dfoff = pd.read_csv(os.path.join(DATA_ROOT,'train_offline.csv'))\n",
    "dftest = pd.read_csv(os.path.join(DATA_ROOT,'test_offline.csv'))\n",
    "dftest = dftest[~dftest.Coupon_id.isna()]\n",
    "dftest.reset_index(drop=True, inplace=True)\n",
    "# 檢查 DataFrame 空缺值的狀態\n",
    "def na_check(df_data):\n",
    "    data_na = (df_data.isnull().sum() / len(df_data)) * 100\n",
    "    data_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)\n",
    "    missing_data = pd.DataFrame({'Missing Ratio' :data_na})\n",
    "    display(missing_data.head(10))\n",
    "# Creat target label \n",
    "\"\"\"\n",
    "According to the definition, \n",
    "1) buy with coupon within (include) 15 days ==> 1\n",
    "2) buy with coupon but out of 15 days ==> 0\n",
    "3) buy without coupon ==> -1 (we don't care)\n",
    "\"\"\"\n",
    "def label(row):\n",
    "    if np.isnan(row['Coupon_id']):\n",
    "        return -1\n",
    "    if not np.isnan(row['Date']):\n",
    "        td = pd.to_datetime(row['Date'], format='%Y%m%d') -  pd.to_datetime(row['Date_received'], format='%Y%m%d')\n",
    "        if td <= pd.Timedelta(15, 'D'):\n",
    "            return 1\n",
    "    return 0\n",
    "#從字串轉成浮點數\n",
    "def convertRate(row):\n",
    "    \"\"\"Convert discount to rate\"\"\"\n",
    "    if row == 'null':\n",
    "        return 1.0\n",
    "    elif ':' in row:\n",
    "        rows = row.split(':')\n",
    "        return 1.0 - float(rows[1])/float(rows[0])\n",
    "    else:\n",
    "        return float(row)\n",
    "# Generate features - weekday acquired coupon\n",
    "def getWeekday(row):\n",
    "    if (np.isnan(row)) or (row==-1):\n",
    "        return row\n",
    "    else:\n",
    "        return pd.to_datetime(row, format = \"%Y%m%d\").dayofweek+1 # add one to make it from 0~6-> 1~7\n",
    "#增加特徵期望值   \n",
    "#1.距離越遠 期望值越低 \n",
    "#2.折扣越高 期望值越低\n",
    "def getExpecteValue(row):\n",
    "    result = 0.0\n",
    "    temp = 0.0\n",
    "    if row['Distance'] == 0:\n",
    "       temp = 0.1\n",
    "    else:\n",
    "       temp = row['Distance']\n",
    "    result = (1/row['Discount_rate'])*(1/temp)\n",
    "    return result  \n",
    "#產生lable\n",
    "dfoff[\"label\"] = dfoff.apply(label, axis=1)\n",
    "#-------------------------------------------\n",
    "#label = -1 刪掉數據\n",
    "train_x = dfoff[dfoff[\"label\"] != -1].drop([\"Date\",\"label\"], axis = 1).copy()\n",
    "train_y = dfoff[\"label\"][dfoff[\"label\"] != -1].copy()\n",
    "#-------------------------------------------------\n",
    "#合併測試集及訓練集\n",
    "df = pd.concat([train_x, dftest],sort=True)\n",
    "#--------------------------------------------\n",
    "#資料預處理 補缺失值及特徵轉換\n",
    "df[\"Discount_rate\"] = df['Discount_rate'].astype('str').apply(convertRate)\n",
    "df[\"Distance\"][df[\"Distance\"].isnull()] = df[\"Distance\"][~df[\"Distance\"].isnull()].median()\n",
    "df[\"Discount_rate\"][df[\"Discount_rate\"]< 0.7] = df[\"Discount_rate\"][~df[\"Discount_rate\"].isnull()].median()\n",
    "df['weekday'] = df['Date_received'].apply(getWeekday)\n",
    "df['Expect_Value'] = df.apply(getExpecteValue,axis = 1)\n",
    "#------------------------------------------------------\n",
    "#以下為離散化 AND ONE_HOT_ENCODING \n",
    "df[\"C_W_D\"] = pd.cut(df[\"Distance\"], [0,1,2,4,9,10])\n",
    "df = pd.get_dummies(df, columns = [\"C_W_D\"], prefix=\"C_W_D\")\n",
    "df[\"C_W_DR\"] = pd.cut(df[\"Discount_rate\"], [0.3,0.72,0.82,0.85,0.87,0.9,1.0])\n",
    "df = pd.get_dummies(df, columns = [\"C_W_DR\"], prefix=\"C_W_DR\")\n",
    "df['received_month'] = df['Date_received'].apply(lambda x: pd.to_datetime(x, format = \"%Y%m%d\").month)\n",
    "df = pd.get_dummies(df, columns = [\"received_month\"], prefix=\"received_month\")\n",
    "df['received_day'] = df['Date_received'].apply(lambda x: pd.to_datetime(x, format = \"%Y%m%d\").day)\n",
    "df[\"C_R_D\"] = pd.cut(df[\"received_day\"], [0,5,10,15,20,25,30])\n",
    "df = pd.get_dummies(df, columns = [\"C_R_D\"], prefix=\"C_R_D\")\n",
    "#---------------------------\n",
    "#特徵交叉\n",
    "Expect_Value_mean = df.groupby(['User_id'])['Expect_Value'].mean().reset_index()\n",
    "Expect_Value_mean.columns = ['User_id', 'Expect_Value_mean']\n",
    "Discount_rate_Usermean = df.groupby(['User_id'])['Discount_rate'].mean().reset_index()\n",
    "Discount_rate_Usermean.columns = ['User_id','Discount_rate_Usermean']\n",
    "temp = pd.merge(Discount_rate_Usermean, Expect_Value_mean, how='left', on=['User_id'])\n",
    "df = pd.merge(df,temp, how='left', on=['User_id'])\n",
    "Expect_Value_sum = df.groupby(['Date_received','Coupon_id'])['Expect_Value'].sum().reset_index()\n",
    "Expect_Value_sum.columns = ['Date_received','Coupon_id','Expect_Value_sum']\n",
    "Expect_Value_mean = df.groupby(['Date_received','Coupon_id'])['Expect_Value'].mean().reset_index()\n",
    "Expect_Value_mean.columns = ['Date_received','Coupon_id','Expect_Value_sum_user']\n",
    "Expect_Value_mean_user = df.groupby(['User_id','Coupon_id'])['Expect_Value'].mean().reset_index()\n",
    "Expect_Value_mean_user.columns = ['User_id','Coupon_id','Expect_Value_mean_user']\n",
    "Expect_Value_sum_user = df.groupby(['User_id','Coupon_id'])['Expect_Value'].sum().reset_index()\n",
    "Expect_Value_sum_user.columns = ['User_id','Coupon_id','Expect_Value_sum']\n",
    "temp1 = pd.merge(Expect_Value_mean,Expect_Value_sum, how='left', on=['Date_received','Coupon_id'])\n",
    "temp2 = pd.merge(Expect_Value_mean_user,Expect_Value_sum_user, how='left', on=['User_id','Coupon_id'])\n",
    "df = pd.merge(df,temp1, how='left', on=['Date_received','Coupon_id'])\n",
    "df = pd.merge(df,temp2, how='left', on=['User_id','Coupon_id'])\n",
    "Discount_rate_MerchantSum = df.groupby(['Merchant_id'])['Discount_rate'].sum().reset_index()\n",
    "Discount_rate_MerchantSum.columns = ['Merchant_id', 'Discount_rate_MerchantSum']\n",
    "df =pd.merge(df, Discount_rate_MerchantSum, how='left', on=['Merchant_id'])\n",
    "Discount_rate_MerchantMean = df.groupby(['Merchant_id'])['Discount_rate'].mean().reset_index()\n",
    "Discount_rate_MerchantMean.columns = ['Merchant_id', 'Discount_rate_MerchantMean']\n",
    "df =pd.merge(df, Discount_rate_MerchantMean, how='left', on=['Merchant_id'])\n",
    "#-------------------------------------------------------------\n",
    "# 將資料最大最小化\n",
    "df = MinMaxScaler().fit_transform(df)\n",
    "train_num = train_y.shape[0]\n",
    "real_train_x = df[:train_num]\n",
    "real_test_x = df[train_num:]\n",
    "#以下為用隨機森林做特徵選取\n",
    "# 建立模型 (使用 20 顆樹，每棵樹的最大深度為 4)\n",
    "# clf = RandomForestClassifier(n_estimators=20,max_depth = 4)\n",
    "\n",
    "# 訓練模型\n",
    "# clf.fit(real_train_x, train_y)\n",
    "# threshold = 0.1\n",
    "# importances = clf.feature_importances_\n",
    "# print(importances)\n",
    "# df_select = df[:, importances > threshold]\n",
    "# print(df_select.shape[1])\n",
    "# real_train_x= df_select[:train_num]\n",
    "# real_test_x = df_select[train_num:]\n",
    "#-------------------------------------------\n",
    "\n",
    "#以下為模型擬合資料+輸出預測結果\n",
    "# lr = LogisticRegression(tol=0.001, penalty='l2', fit_intercept=True, C=1.0)\n",
    "# rf = RandomForestClassifier(n_estimators=100, min_samples_split=2, min_samples_leaf=1, \n",
    "#                             max_features='sqrt', max_depth=6, bootstrap=True)\n",
    "xgb = XGBClassifier()\n",
    "# meta_estimator = XGBClassifier(n_estimators = 50,min_child_weight = 3, max_depth = 3,subsample = 0.8,learning_rate = 0.3)\n",
    "# sclf = StackingClassifier(classifiers=[lr, xgb, rf],\n",
    "#                           use_probas=True,\n",
    "#                           average_probas=False,\n",
    "#                           meta_classifier=meta_estimator)\n",
    "# sclf.fit(real_train_x, train_y)\n",
    "# y_test_pred = sclf.predict_proba(real_test_x)\n",
    "modelfit = xgb.fit(real_train_x,train_y)\n",
    "# accuracy = np.mean(cross_val_score(modelfit, real_train_x, train_y, cv=3, scoring='accuracy'))\n",
    "# print(\"XGBClassifier accuracy: \", accuracy)\n",
    "y_test_pred = modelfit.predict_proba(real_test_x)\n",
    "test1 = dftest.loc[:,['Discount_rate','Expect_Value']].copy()\n",
    "test1['pred_prob'] = y_test_pred[:, 1]\n",
    "output = pd.concat((dftest[[\"User_id\", \"Coupon_id\", \"Date_received\"]], test1[\"pred_prob\"]), axis=1)\n",
    "output.loc[:, \"User_id\"] = output[\"User_id\"].apply(lambda x:str(int(x)))\n",
    "output.loc[:, \"Coupon_id\"] = output[\"Coupon_id\"].apply(lambda x:str(int(x)))\n",
    "output.loc[:, \"Date_received\"] = output[\"Date_received\"].apply(lambda x:str(int(x)))\n",
    "output[\"uid\"] = output[[\"User_id\", \"Coupon_id\", \"Date_received\"]].apply(lambda x: '_'.join(x.values), axis=1)\n",
    "output.reset_index(drop=True, inplace=True)\n",
    "out = output.groupby(\"uid\", as_index=False).mean()\n",
    "out = out[[\"uid\", \"pred_prob\"]]\n",
    "out.columns = [\"uid\", \"label\"]\n",
    "out.to_csv(\"xgb_MinMaxScaler_Modify3.csv\", header=[\"uid\", \"label\"], index=False) # submission format\n",
    "#-----------------------------------------------------------------------------------\n",
    "#以下為資料視覺化+數據清洗\n",
    "# df = pd.merge(df, Expect_Value_mean, how='left', on=['User_id'])\n",
    "# df.head(10)\n",
    "# print(df['User_id'].nunique())\n",
    "# print(df['Merchant_id'].nunique())\n",
    "# print(df['Coupon_id'].nunique())\n",
    "# df['Expect_Value_log'] = np.log1p(df[\"Expect_Value\"])\n",
    "# df['Distance_log'] = np.log1p(df['Distance'])\n",
    "# df['received_month'].hist()\n",
    "# plt.show()\n",
    "# df['received_day'].hist()\n",
    "# plt.show()\n",
    "# plt.boxplot(df['Expect_Value_log'],\n",
    "#             notch=True,  # notch shape\n",
    "#             sym='bs',     # blue squares for outliers\n",
    "#             vert=True,   # vertical box aligmnent\n",
    "#             patch_artist=True)   # fill with color \n",
    "# plt.title('Expect_Value_log') \n",
    "# plt.show()\n",
    "\n",
    "\n",
    "#遇到均值編碼時 連id也一起算平均 所以merge不起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "xgb_MinMaxScaler_Modify2 = pd.read_csv('xgb_MinMaxScaler_Modify2.csv')\n",
    "StackingClassifier_Modify = pd.read_csv('StackingClassifier_Modify.csv')\n",
    "xgb_StandardScaler_modify = pd.read_csv('xgb_StandardScaler_modify.csv')\n",
    "\n",
    "blend = xgb_MinMaxScaler_Modify2.copy()\n",
    "blend['label'] =0.2*xgb_MinMaxScaler_Modify2['label'] + 0.5*StackingClassifier_Modify['label'] + 0.3*xgb_StandardScaler_modify['label']\n",
    "blend.to_csv(\"blend.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
